{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coated-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sublime-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu for training if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-bolivia",
   "metadata": {},
   "source": [
    "### Model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "christian-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiple_layer_fc_network(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(multiple_layer_fc_network, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-sphere",
   "metadata": {},
   "source": [
    "### Dimensionality checking with random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charged-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64, 784)\n",
    "model = multiple_layer_fc_network(input_size=784, num_classes=10)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-fortune",
   "metadata": {},
   "source": [
    "### Global Variables Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "western-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "num_workers = 16 #multiprocessing.cpu_count() \n",
    "\n",
    "early_stopping_criteria= 5\n",
    "\n",
    "val_proportion = 0.2\n",
    "\n",
    "data_dir = \"/mnt/addtional_data_ssd/pytorch_tutorials/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-overhead",
   "metadata": {},
   "source": [
    "### Set up Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aboriginal-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number samples in train: 48000\n",
      "number samples in val: 12000\n",
      "number samples in test: 10000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.FashionMNIST(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset  = datasets.FashionMNIST(root=data_dir, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "val_size = int(val_proportion*len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "print(f\"number samples in train: {train_size}\")\n",
    "print(f\"number samples in val: {val_size}\")\n",
    "print(f\"number samples in test: {test_size}\")\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "multiple-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "express-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image has shape:\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# check the dimensions of the data\n",
    "image, target = train_dataset[0]\n",
    "print(\"input image has shape:\")\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-modern",
   "metadata": {},
   "source": [
    "### Model Initialization, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sharp-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = multiple_layer_fc_network(input_size=input_size, num_classes=num_classes)\n",
    "clf.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-voluntary",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "emerging-spiritual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0 step 199] running training loss: 0.806\n",
      "[epoch 0 step 399] running training loss: 0.539\n",
      "[epoch 0 step 599] running training loss: 0.496\n",
      "[epoch 0] val loss: 0.444, accuracy: 84.483%\n",
      "current best validation loss: 0.444\n",
      "--------------------------------------------\n",
      "[epoch 1 step 199] running training loss: 0.448\n",
      "[epoch 1 step 399] running training loss: 0.439\n",
      "[epoch 1 step 599] running training loss: 0.424\n",
      "[epoch 1] val loss: 0.420, accuracy: 84.708%\n",
      "current best validation loss: 0.420\n",
      "--------------------------------------------\n",
      "[epoch 2 step 199] running training loss: 0.390\n",
      "[epoch 2 step 399] running training loss: 0.384\n",
      "[epoch 2 step 599] running training loss: 0.378\n",
      "[epoch 2] val loss: 0.395, accuracy: 86.017%\n",
      "current best validation loss: 0.395\n",
      "--------------------------------------------\n",
      "[epoch 3 step 199] running training loss: 0.351\n",
      "[epoch 3 step 399] running training loss: 0.362\n",
      "[epoch 3 step 599] running training loss: 0.354\n",
      "[epoch 3] val loss: 0.374, accuracy: 86.467%\n",
      "current best validation loss: 0.374\n",
      "--------------------------------------------\n",
      "[epoch 4 step 199] running training loss: 0.336\n",
      "[epoch 4 step 399] running training loss: 0.333\n",
      "[epoch 4 step 599] running training loss: 0.339\n",
      "[epoch 4] val loss: 0.361, accuracy: 87.158%\n",
      "current best validation loss: 0.361\n",
      "--------------------------------------------\n",
      "[epoch 5 step 199] running training loss: 0.321\n",
      "[epoch 5 step 399] running training loss: 0.313\n",
      "[epoch 5 step 599] running training loss: 0.324\n",
      "[epoch 5] val loss: 0.346, accuracy: 87.575%\n",
      "current best validation loss: 0.346\n",
      "--------------------------------------------\n",
      "[epoch 6 step 199] running training loss: 0.297\n",
      "[epoch 6 step 399] running training loss: 0.307\n",
      "[epoch 6 step 599] running training loss: 0.303\n",
      "[epoch 6] val loss: 0.343, accuracy: 87.658%\n",
      "current best validation loss: 0.343\n",
      "--------------------------------------------\n",
      "[epoch 7 step 199] running training loss: 0.296\n",
      "[epoch 7 step 399] running training loss: 0.289\n",
      "[epoch 7 step 599] running training loss: 0.290\n",
      "[epoch 7] val loss: 0.353, accuracy: 87.233%\n",
      "validation loss stops decreasing for 1 epoch\n",
      "--------------------------------------------\n",
      "[epoch 8 step 199] running training loss: 0.284\n",
      "[epoch 8 step 399] running training loss: 0.272\n",
      "[epoch 8 step 599] running training loss: 0.285\n",
      "[epoch 8] val loss: 0.341, accuracy: 87.583%\n",
      "current best validation loss: 0.341\n",
      "--------------------------------------------\n",
      "[epoch 9 step 199] running training loss: 0.263\n",
      "[epoch 9 step 399] running training loss: 0.275\n",
      "[epoch 9 step 599] running training loss: 0.283\n",
      "[epoch 9] val loss: 0.331, accuracy: 87.983%\n",
      "current best validation loss: 0.331\n",
      "--------------------------------------------\n",
      "[epoch 10 step 199] running training loss: 0.262\n",
      "[epoch 10 step 399] running training loss: 0.263\n",
      "[epoch 10 step 599] running training loss: 0.267\n",
      "[epoch 10] val loss: 0.321, accuracy: 88.283%\n",
      "current best validation loss: 0.321\n",
      "--------------------------------------------\n",
      "[epoch 11 step 199] running training loss: 0.252\n",
      "[epoch 11 step 399] running training loss: 0.255\n",
      "[epoch 11 step 599] running training loss: 0.256\n",
      "[epoch 11] val loss: 0.320, accuracy: 88.483%\n",
      "current best validation loss: 0.320\n",
      "--------------------------------------------\n",
      "[epoch 12 step 199] running training loss: 0.244\n",
      "[epoch 12 step 399] running training loss: 0.259\n",
      "[epoch 12 step 599] running training loss: 0.248\n",
      "[epoch 12] val loss: 0.324, accuracy: 88.483%\n",
      "validation loss stops decreasing for 1 epoch\n",
      "--------------------------------------------\n",
      "[epoch 13 step 199] running training loss: 0.253\n",
      "[epoch 13 step 399] running training loss: 0.248\n",
      "[epoch 13 step 599] running training loss: 0.236\n",
      "[epoch 13] val loss: 0.327, accuracy: 88.300%\n",
      "validation loss stops decreasing for 2 epoch\n",
      "--------------------------------------------\n",
      "[epoch 14 step 199] running training loss: 0.236\n",
      "[epoch 14 step 399] running training loss: 0.243\n",
      "[epoch 14 step 599] running training loss: 0.242\n",
      "[epoch 14] val loss: 0.328, accuracy: 88.233%\n",
      "validation loss stops decreasing for 3 epoch\n",
      "--------------------------------------------\n",
      "[epoch 15 step 199] running training loss: 0.225\n",
      "[epoch 15 step 399] running training loss: 0.241\n",
      "[epoch 15 step 599] running training loss: 0.229\n",
      "[epoch 15] val loss: 0.318, accuracy: 88.925%\n",
      "current best validation loss: 0.318\n",
      "--------------------------------------------\n",
      "[epoch 16 step 199] running training loss: 0.223\n",
      "[epoch 16 step 399] running training loss: 0.223\n",
      "[epoch 16 step 599] running training loss: 0.228\n",
      "[epoch 16] val loss: 0.311, accuracy: 88.950%\n",
      "current best validation loss: 0.311\n",
      "--------------------------------------------\n",
      "[epoch 17 step 199] running training loss: 0.212\n",
      "[epoch 17 step 399] running training loss: 0.207\n",
      "[epoch 17 step 599] running training loss: 0.217\n",
      "[epoch 17] val loss: 0.311, accuracy: 89.067%\n",
      "current best validation loss: 0.311\n",
      "--------------------------------------------\n",
      "[epoch 18 step 199] running training loss: 0.219\n",
      "[epoch 18 step 399] running training loss: 0.209\n",
      "[epoch 18 step 599] running training loss: 0.210\n",
      "[epoch 18] val loss: 0.316, accuracy: 89.275%\n",
      "validation loss stops decreasing for 1 epoch\n",
      "--------------------------------------------\n",
      "[epoch 19 step 199] running training loss: 0.207\n",
      "[epoch 19 step 399] running training loss: 0.210\n",
      "[epoch 19 step 599] running training loss: 0.211\n",
      "[epoch 19] val loss: 0.313, accuracy: 89.342%\n",
      "validation loss stops decreasing for 2 epoch\n",
      "--------------------------------------------\n",
      "[epoch 20 step 199] running training loss: 0.201\n",
      "[epoch 20 step 399] running training loss: 0.198\n",
      "[epoch 20 step 599] running training loss: 0.215\n",
      "[epoch 20] val loss: 0.312, accuracy: 89.083%\n",
      "validation loss stops decreasing for 3 epoch\n",
      "--------------------------------------------\n",
      "[epoch 21 step 199] running training loss: 0.186\n",
      "[epoch 21 step 399] running training loss: 0.204\n",
      "[epoch 21 step 599] running training loss: 0.203\n",
      "[epoch 21] val loss: 0.322, accuracy: 88.750%\n",
      "validation loss stops decreasing for 4 epoch\n",
      "--------------------------------------------\n",
      "[epoch 22 step 199] running training loss: 0.189\n",
      "[epoch 22 step 399] running training loss: 0.205\n",
      "[epoch 22 step 599] running training loss: 0.192\n",
      "[epoch 22] val loss: 0.318, accuracy: 89.008%\n",
      "validation loss stops decreasing for 5 epoch\n",
      "early stopping criteria reached\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "early_stopping_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # transfer data to gpu if available\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        #print(images.shape, targets.shape)\n",
    "        \n",
    "        # reshape the images to proper shape\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.reshape(batch_size, -1)\n",
    "        \n",
    "        # forward pass\n",
    "        preds = clf(images)\n",
    "        loss = criterion(preds, targets)\n",
    "        \n",
    "        # back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update model params\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # print training stat every 200 steps\n",
    "        if ((batch_idx+1) % 200==0):\n",
    "            print(f\"[epoch {epoch} step {batch_idx}] running training loss: {running_train_loss/200:.3f}\")\n",
    "            running_train_loss = 0.0\n",
    "            \n",
    "    # evaluate on the validation set every epoch\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        num_val_samples = 0 \n",
    "        correct = 0\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            batch_size = images.shape[0]\n",
    "            images = images.reshape(batch_size, -1)\n",
    "            preds = clf(images)\n",
    "            loss = criterion(preds, targets)\n",
    "            val_loss += loss.item() * images.shape[0]\n",
    "            num_val_samples += batch_size\n",
    "            # calculate accuracy\n",
    "            preds = clf(images)\n",
    "            _, pred_labels = torch.max(preds, dim=1)\n",
    "            correct += (pred_labels==targets).sum().item()\n",
    "    epoch_val_loss= val_loss/num_val_samples\n",
    "    print(f\"[epoch {epoch}] val loss: {epoch_val_loss:.3f}, accuracy: {100*correct/num_val_samples:.3f}%\")\n",
    "    \n",
    "    # early stopping\n",
    "    if (epoch_val_loss < best_val_loss):\n",
    "        best_val_loss = epoch_val_loss\n",
    "        early_stopping_step = 0\n",
    "        print(f\"current best validation loss: {best_val_loss:.3f}\")\n",
    "    else:\n",
    "        early_stopping_step += 1\n",
    "        print(f\"validation loss stops decreasing for {early_stopping_step} epoch\")\n",
    "        if (early_stopping_step==early_stopping_criteria):\n",
    "            print(\"early stopping criteria reached\")\n",
    "            break\n",
    "    print(\"--------------------------------------------\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-honey",
   "metadata": {},
   "source": [
    "### Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accompanied-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss: 0.359, testing accuracy: 87.880%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    num_test_samples = 0 \n",
    "    correct = 0\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.reshape(batch_size, -1)\n",
    "        preds = clf(images)\n",
    "        loss = criterion(preds, targets)\n",
    "        test_loss += loss.item() * images.shape[0]\n",
    "        num_test_samples += batch_size\n",
    "        # calculate accuracy\n",
    "        preds = clf(images)\n",
    "        _, pred_labels = torch.max(preds, dim=1)\n",
    "        correct += (pred_labels==targets).sum().item()\n",
    "print(f\"testing loss: {test_loss/num_test_samples:.3f}, testing accuracy: {100*correct/num_test_samples:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-emphasis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dev_kernel",
   "language": "python",
   "name": "torch_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
